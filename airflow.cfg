[core]
# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = Asia/Bangkok

# The executor class that airflow should use. Choices include
# ``SequentialExecutor``, ``LocalExecutor``, ``CeleryExecutor``, ``DaskExecutor``,
# ``KubernetesExecutor``, ``CeleryKubernetesExecutor`` or the
# full import path to the class when using a custom executor.
#
# NOTE:
#   LocalExecutor: parallelize task instances locally.
executor = LocalExecutor

# This defines the maximum number of task instances that can run concurrently in Airflow
# regardless of scheduler count and worker count. Generally, this value is reflective of
# the number of task instances with the running state in the metadata database.
parallelism = 32

# Secret key to save connection passwords in the db
fernet_key = 'ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg='

load_examples = False

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# Path to the folder containing Airflow plugins
plugins_folder = /opt/airflow/plugins

lazy_load_plugins = False

# Hide sensitive Variables or Connection extra json keys from UI and task logs
# when set to True (Connection passwords are always hidden in logs)
hide_sensitive_var_conn_fields = False

[database]
# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engines.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The encoding for the databases
sql_engine_encoding = utf-8

# If SqlAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool. 0 indicates no limit.
sql_alchemy_pool_size = 5

[logging]
# The folder where airflow should store its log files.
# This path must be absolute.
# There are a few existing configurations that assume this is set to the default.
# If you choose to override this you may need to update the dag_processor_manager_log_location and
# dag_processor_manager_log_location settings as well.
base_log_folder = /opt/airflow/logs

[api]
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

[secrets]
# [NOTE]: See documentation to see all the different options for backend_kwargs
#   https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/secrets-backends/aws-secrets-manager.html
#   https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/secrets-backends/aws-ssm-parameter-store.html
backend = airflow.secrets.local_filesystem.LocalFilesystemBackend
backend_kwargs = {
        "connections_file_path": "/opt/secrets/connections.yaml",
        "variables_file_path": "/opt/secrets/variables.yaml"
    }

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

enable_health_check = True

# How often (in seconds) to check for stale DAGs (DAGs which are no longer
# present in the expected files) which should be deactivated, as well as
# datasets that are no longer referenced and should be marked as orphaned.
parsing_cleanup_interval = 30

# How often (in seconds) to scan the DAGs directory for new files.
# Default to 5 minutes.
dag_dir_list_interval = 15

# Keeping this number low will increase CPU usage.
min_file_process_interval = 15

[webserver]
# FIXME: On production, we need to change this config to FALSE
expose_config = True

[email]
email_backend = airflow.utils.email.send_email_smtp
# email_conn_id = smtp_custom
# from_email = airflow_admin@email.io
html_content_template = /opt/airflow/include/templates/email/notify/content.html
subject_template = /opt/airflow/include/templates/email/notify/subject.html

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here.
smtp_host = mailhog
smtp_user = airflow_admin@email.io
smtp_password = mailhog#dummy
smtp_port = 1025
smtp_starttls = False
smtp_ssl = False
smtp_mail_from = airflow_admin@email.io
